{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2f79feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))  # sobe um nível a partir da pasta models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2489d68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "\n",
    "from copy import deepcopy\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from val.Wrapper import ForwardFeatureSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8420b344",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_100_100_100_classifier(nn.Module):\n",
    "    def __init__(self, input_len):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer1 = nn.Linear(input_len, 100)  \n",
    "        self.relu = nn.ReLU()                          \n",
    "        self.hidden_layer_relu = nn.Sequential(\n",
    "            nn.Linear(100,100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100,100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100,100),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer2 = nn.Linear(100, 30)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.hidden_layer_relu(x)\n",
    "        logits = self.layer2(x)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a83dee9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# abrindo os dados de treinamento\n",
    "df = pd.read_csv(\"./../ansatz_result/data.csv\")\n",
    "X = df.drop(columns=[\"target\"]).to_numpy()\n",
    "y = pd.DataFrame(df['target'].apply(ast.literal_eval).tolist()).to_numpy()\n",
    "\n",
    "y_best_ansatz = np.argmax(y, axis=1)  \n",
    "\n",
    "# criando o kfold\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a906636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "050a4ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementação de um critério de parada para o modelo parar no ponto \"ótimo\"\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0.0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.early_stop = False\n",
    "        \n",
    "    def __call__(self, val_loss):\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f0e2114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Função para treinar um fold\n",
    "def train_fold(model, train_loader, val_loader, criterion, optimizer, n_epochs=1000000, patience = 10, min_delta = 0.0):\n",
    "    model.to(device)\n",
    "    early_stopping = EarlyStopping(patience=patience, min_delta=min_delta)\n",
    "    best_model = None\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # Treinamento\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            y_batch = y_batch.squeeze()  # Converte (batch_size, 1) para (batch_size,)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validação\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for X_val, y_val in val_loader:\n",
    "                X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "                y_val = y_val.squeeze()  # Garante que y_val é 1D\n",
    "                \n",
    "                outputs = model(X_val)\n",
    "                val_loss += criterion(outputs, y_val).item()  \n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                correct += (predicted == y_val).sum().item()\n",
    "                total += y_val.size(0)\n",
    "        \n",
    "        \n",
    "        val_acc = correct / total\n",
    "        val_loss /= len(val_loader)\n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{n_epochs} - Train Loss: {train_loss:.4f} - Val Loss: {val_loss:.4f} - Val Acc: {val_acc:.4f}')\n",
    "        \n",
    "        # Early Stopping\n",
    "        early_stopping(val_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "            \n",
    "        # Salvar melhor modelo\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_model = deepcopy(model.state_dict())\n",
    "    \n",
    "    return best_model, best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15e641cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 1/3 ===\n",
      "Epoch 1/1000000 - Train Loss: 3.0220 - Val Loss: 3.0197 - Val Acc: 0.0583\n",
      "Epoch 2/1000000 - Train Loss: 2.6556 - Val Loss: 2.8120 - Val Acc: 0.3000\n",
      "Epoch 3/1000000 - Train Loss: 2.4528 - Val Loss: 2.6871 - Val Acc: 0.3000\n",
      "Epoch 4/1000000 - Train Loss: 2.4343 - Val Loss: 2.7326 - Val Acc: 0.3000\n",
      "Epoch 5/1000000 - Train Loss: 2.3633 - Val Loss: 2.6949 - Val Acc: 0.2750\n",
      "Epoch 6/1000000 - Train Loss: 2.2957 - Val Loss: 2.7392 - Val Acc: 0.2583\n",
      "Epoch 7/1000000 - Train Loss: 2.2756 - Val Loss: 2.7175 - Val Acc: 0.2583\n",
      "Epoch 8/1000000 - Train Loss: 2.2801 - Val Loss: 2.7042 - Val Acc: 0.2583\n",
      "Epoch 9/1000000 - Train Loss: 2.2597 - Val Loss: 2.6920 - Val Acc: 0.2583\n",
      "Epoch 10/1000000 - Train Loss: 2.2276 - Val Loss: 2.7048 - Val Acc: 0.2583\n",
      "Epoch 11/1000000 - Train Loss: 2.2334 - Val Loss: 2.7191 - Val Acc: 0.2583\n",
      "Epoch 12/1000000 - Train Loss: 2.2058 - Val Loss: 2.7372 - Val Acc: 0.2750\n",
      "Epoch 13/1000000 - Train Loss: 2.2482 - Val Loss: 2.7284 - Val Acc: 0.3000\n",
      "Epoch 14/1000000 - Train Loss: 2.2512 - Val Loss: 2.7714 - Val Acc: 0.2750\n",
      "Epoch 15/1000000 - Train Loss: 2.1794 - Val Loss: 2.7518 - Val Acc: 0.3000\n",
      "Epoch 16/1000000 - Train Loss: 2.1821 - Val Loss: 2.7680 - Val Acc: 0.3000\n",
      "Epoch 17/1000000 - Train Loss: 2.1912 - Val Loss: 2.7195 - Val Acc: 0.2667\n",
      "Epoch 18/1000000 - Train Loss: 2.2307 - Val Loss: 2.7399 - Val Acc: 0.2917\n",
      "Early stopping at epoch 18\n",
      "\n",
      "=== Fold 2/3 ===\n",
      "Epoch 1/1000000 - Train Loss: 3.0490 - Val Loss: 2.6693 - Val Acc: 0.2917\n",
      "Epoch 2/1000000 - Train Loss: 2.6856 - Val Loss: 2.7851 - Val Acc: 0.2917\n",
      "Epoch 3/1000000 - Train Loss: 2.6345 - Val Loss: 2.5558 - Val Acc: 0.2917\n",
      "Epoch 4/1000000 - Train Loss: 2.5000 - Val Loss: 2.4971 - Val Acc: 0.2917\n",
      "Epoch 5/1000000 - Train Loss: 2.3838 - Val Loss: 2.4454 - Val Acc: 0.3250\n",
      "Epoch 6/1000000 - Train Loss: 2.3461 - Val Loss: 2.4845 - Val Acc: 0.3250\n",
      "Epoch 7/1000000 - Train Loss: 2.3362 - Val Loss: 2.4694 - Val Acc: 0.3250\n",
      "Epoch 8/1000000 - Train Loss: 2.3406 - Val Loss: 2.4546 - Val Acc: 0.3250\n",
      "Epoch 9/1000000 - Train Loss: 2.2985 - Val Loss: 2.4753 - Val Acc: 0.3250\n",
      "Epoch 10/1000000 - Train Loss: 2.2607 - Val Loss: 2.4587 - Val Acc: 0.3250\n",
      "Epoch 11/1000000 - Train Loss: 2.2552 - Val Loss: 2.5020 - Val Acc: 0.3250\n",
      "Epoch 12/1000000 - Train Loss: 2.2736 - Val Loss: 2.4445 - Val Acc: 0.3667\n",
      "Epoch 13/1000000 - Train Loss: 2.2572 - Val Loss: 2.4415 - Val Acc: 0.3667\n",
      "Epoch 14/1000000 - Train Loss: 2.3026 - Val Loss: 2.5035 - Val Acc: 0.3667\n",
      "Epoch 15/1000000 - Train Loss: 2.2761 - Val Loss: 2.4854 - Val Acc: 0.3667\n",
      "Epoch 16/1000000 - Train Loss: 2.2421 - Val Loss: 2.5175 - Val Acc: 0.3583\n",
      "Epoch 17/1000000 - Train Loss: 2.2246 - Val Loss: 2.5072 - Val Acc: 0.3583\n",
      "Epoch 18/1000000 - Train Loss: 2.2592 - Val Loss: 2.4735 - Val Acc: 0.3583\n",
      "Epoch 19/1000000 - Train Loss: 2.2204 - Val Loss: 2.4767 - Val Acc: 0.3667\n",
      "Epoch 20/1000000 - Train Loss: 2.2002 - Val Loss: 2.5289 - Val Acc: 0.3667\n",
      "Epoch 21/1000000 - Train Loss: 2.2081 - Val Loss: 2.4859 - Val Acc: 0.3583\n",
      "Epoch 22/1000000 - Train Loss: 2.1934 - Val Loss: 2.5173 - Val Acc: 0.3667\n",
      "Epoch 23/1000000 - Train Loss: 2.1987 - Val Loss: 2.5309 - Val Acc: 0.3667\n",
      "Epoch 24/1000000 - Train Loss: 2.1865 - Val Loss: 2.4899 - Val Acc: 0.3750\n",
      "Epoch 25/1000000 - Train Loss: 2.1253 - Val Loss: 2.5116 - Val Acc: 0.3750\n",
      "Epoch 26/1000000 - Train Loss: 2.1565 - Val Loss: 2.5682 - Val Acc: 0.3500\n",
      "Epoch 27/1000000 - Train Loss: 2.2062 - Val Loss: 2.5225 - Val Acc: 0.3667\n",
      "Epoch 28/1000000 - Train Loss: 2.1498 - Val Loss: 2.5098 - Val Acc: 0.3667\n",
      "Early stopping at epoch 28\n",
      "\n",
      "=== Fold 3/3 ===\n",
      "Epoch 1/1000000 - Train Loss: 3.0852 - Val Loss: 2.8351 - Val Acc: 0.2833\n",
      "Epoch 2/1000000 - Train Loss: 2.6593 - Val Loss: 2.5829 - Val Acc: 0.2833\n",
      "Epoch 3/1000000 - Train Loss: 2.5008 - Val Loss: 2.5278 - Val Acc: 0.2833\n",
      "Epoch 4/1000000 - Train Loss: 2.4096 - Val Loss: 2.5179 - Val Acc: 0.3417\n",
      "Epoch 5/1000000 - Train Loss: 2.3619 - Val Loss: 2.5691 - Val Acc: 0.3417\n",
      "Epoch 6/1000000 - Train Loss: 2.3249 - Val Loss: 2.5841 - Val Acc: 0.2833\n",
      "Epoch 7/1000000 - Train Loss: 2.3682 - Val Loss: 2.6128 - Val Acc: 0.2833\n",
      "Epoch 8/1000000 - Train Loss: 2.3102 - Val Loss: 2.6160 - Val Acc: 0.2833\n",
      "Epoch 9/1000000 - Train Loss: 2.2695 - Val Loss: 2.7182 - Val Acc: 0.2833\n",
      "Epoch 10/1000000 - Train Loss: 2.2731 - Val Loss: 2.6652 - Val Acc: 0.3417\n",
      "Epoch 11/1000000 - Train Loss: 2.3160 - Val Loss: 2.6795 - Val Acc: 0.3417\n",
      "Epoch 12/1000000 - Train Loss: 2.2682 - Val Loss: 2.6912 - Val Acc: 0.3417\n",
      "Epoch 13/1000000 - Train Loss: 2.3057 - Val Loss: 2.6497 - Val Acc: 0.3333\n",
      "Epoch 14/1000000 - Train Loss: 2.2586 - Val Loss: 2.6433 - Val Acc: 0.2833\n",
      "Epoch 15/1000000 - Train Loss: 2.2864 - Val Loss: 2.7158 - Val Acc: 0.2750\n",
      "Epoch 16/1000000 - Train Loss: 2.2542 - Val Loss: 2.7222 - Val Acc: 0.2833\n",
      "Epoch 17/1000000 - Train Loss: 2.2358 - Val Loss: 2.7220 - Val Acc: 0.2667\n",
      "Epoch 18/1000000 - Train Loss: 2.2779 - Val Loss: 2.7026 - Val Acc: 0.3333\n",
      "Epoch 19/1000000 - Train Loss: 2.2495 - Val Loss: 2.6503 - Val Acc: 0.3167\n",
      "Early stopping at epoch 19\n",
      "\n",
      "=== Resultados ===\n",
      "Acurácia média: 0.3389 ± 0.0307\n",
      "Acurácias por fold: [0.3, 0.375, 0.3416666666666667]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "    print(f\"\\n=== Fold {fold + 1}/{3} ===\")\n",
    "    \n",
    "    # Dividir e preparar dados\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y_best_ansatz[train_idx], y_best_ansatz[val_idx]\n",
    "    \n",
    "    train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.LongTensor(y_train))\n",
    "    val_dataset = TensorDataset(torch.FloatTensor(X_val), torch.LongTensor(y_val))\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "    \n",
    "    # Inicializar modelo e otimizador\n",
    "    model = MLP_100_100_100_classifier(22)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01) \n",
    "    \n",
    "    # Treinar fold\n",
    "    best_model, best_acc = train_fold(model, train_loader, val_loader, criterion, optimizer, patience=15)\n",
    "    results.append(best_acc)\n",
    "    \n",
    "    # Salvar modelo se necessário\n",
    "    torch.save(best_model, f'./models_salvos/best_model_MLP-100-100-100-classifier_fold{fold+1}.pt')\n",
    "\n",
    "# 7. Resultados finais\n",
    "print(\"\\n=== Resultados ===\")\n",
    "print(f\"Acurácia média: {np.mean(results):.4f} ± {np.std(results):.4f}\")\n",
    "print(f\"Acurácias por fold: {results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f03f52a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP_100_100_100_classifier(\n",
       "  (layer1): Linear(in_features=22, out_features=100, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (hidden_layer_relu): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (5): ReLU()\n",
       "  )\n",
       "  (layer2): Linear(in_features=100, out_features=30, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLP_100_100_100_classifier(22).to(device)\n",
    "#carregando o melhor fold treinado\n",
    "model.load_state_dict(torch.load('.//models_salvos/best_model_MLP-100-100-100-classifier_fold2.pt'))  # Substitua pelo caminho correto\n",
    "model.eval()  # Modo de avaliação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecd2cb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2.3737e+00,  3.2321e-03,  1.9258e+00, -3.9920e+00, -7.8860e+00,\n",
      "        -7.2705e+00,  2.1764e+00,  1.8194e+00, -1.3945e+00, -5.6717e+00,\n",
      "        -3.7648e+00,  7.2861e-01,  8.3512e-01,  1.8855e-01,  2.1325e+00,\n",
      "        -7.4403e+00, -9.5926e-01, -3.9527e+00, -4.9849e+00, -4.1674e+00,\n",
      "        -3.9788e+00,  4.2663e-02, -3.6794e+00, -5.0568e+00, -4.3920e+00,\n",
      "        -3.2086e-01, -4.9165e+00, -4.1407e+00, -5.8382e+00, -4.1937e+00])\n",
      "torch.Size([30])\n"
     ]
    }
   ],
   "source": [
    "input = X[6]\n",
    "input_tensor = torch.FloatTensor(input).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(input_tensor)\n",
    "    print(logits)\n",
    "    print(logits.shape)\n",
    "    probabilities = torch.softmax(logits, dim=0)  # Shape (1, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee8e32d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.1390e-01, 1.9986e-02, 1.3667e-01, 3.6782e-04, 7.4898e-06, 1.3861e-05,\n",
      "        1.7560e-01, 1.2288e-01, 4.9396e-03, 6.8570e-05, 4.6162e-04, 4.1281e-02,\n",
      "        4.5921e-02, 2.4055e-02, 1.6806e-01, 1.1696e-05, 7.6334e-03, 3.8256e-04,\n",
      "        1.3627e-04, 3.0863e-04, 3.7269e-04, 2.0790e-02, 5.0277e-04, 1.2682e-04,\n",
      "        2.4654e-04, 1.4453e-02, 1.4592e-04, 3.1697e-04, 5.8056e-05, 3.0061e-04])\n"
     ]
    }
   ],
   "source": [
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9367f6c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores ansatzes e suas probabilidades:\n",
      "1º: Ansatz 0 - Probabilidade: 0.2139\n",
      "2º: Ansatz 6 - Probabilidade: 0.1756\n",
      "3º: Ansatz 14 - Probabilidade: 0.1681\n"
     ]
    }
   ],
   "source": [
    "# Obter índices e valores das top-k probabilidades (ex.: top-5)\n",
    "k = 3\n",
    "top_k_probs, top_k_indices = torch.topk(probabilities, k=k, dim=0)\n",
    "\n",
    "# Converter para numpy e exibir\n",
    "top_k_probs = top_k_probs.cpu().numpy().flatten()\n",
    "top_k_indices = top_k_indices.cpu().numpy().flatten()\n",
    "\n",
    "print(\"Melhores ansatzes e suas probabilidades:\")\n",
    "for i, (idx, prob) in enumerate(zip(top_k_indices, top_k_probs)):\n",
    "    print(f\"{i+1}º: Ansatz {idx} - Probabilidade: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "861dd29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "selector = ForwardFeatureSelector(\n",
    "    model=lambda input_dim: MLP_100_100_100_classifier(input_dim),\n",
    "    model_type='pytorch',\n",
    "    scoring=accuracy_score,\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    pytorch_optimizer= optim.Adam,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a73bbb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing feature set: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing feature set: [1]\n",
      "Testing feature set: [2]\n",
      "Testing feature set: [3]\n",
      "Testing feature set: [4]\n",
      "Testing feature set: [5]\n",
      "Testing feature set: [6]\n",
      "Testing feature set: [7]\n",
      "Testing feature set: [8]\n",
      "Testing feature set: [9]\n",
      "Testing feature set: [10]\n",
      "Testing feature set: [11]\n",
      "Testing feature set: [12]\n",
      "Testing feature set: [13]\n",
      "Testing feature set: [14]\n",
      "Testing feature set: [15]\n",
      "Testing feature set: [16]\n",
      "Testing feature set: [17]\n",
      "Testing feature set: [18]\n",
      "Testing feature set: [19]\n",
      "Testing feature set: [20]\n",
      "Testing feature set: [21]\n",
      "Selected feature: 13 | Score: 0.3389\n",
      "Testing feature set: [13, 0]\n",
      "Testing feature set: [13, 1]\n",
      "Testing feature set: [13, 2]\n",
      "Testing feature set: [13, 3]\n",
      "Testing feature set: [13, 4]\n",
      "Testing feature set: [13, 5]\n",
      "Testing feature set: [13, 6]\n",
      "Testing feature set: [13, 7]\n",
      "Testing feature set: [13, 8]\n",
      "Testing feature set: [13, 9]\n",
      "Testing feature set: [13, 10]\n",
      "Testing feature set: [13, 11]\n",
      "Testing feature set: [13, 12]\n",
      "Testing feature set: [13, 14]\n",
      "Testing feature set: [13, 15]\n",
      "Testing feature set: [13, 16]\n",
      "Testing feature set: [13, 17]\n",
      "Testing feature set: [13, 18]\n",
      "Testing feature set: [13, 19]\n",
      "Testing feature set: [13, 20]\n",
      "Testing feature set: [13, 21]\n",
      "Selected feature: 18 | Score: 0.3611\n",
      "Testing feature set: [13, 18, 0]\n",
      "Testing feature set: [13, 18, 1]\n",
      "Testing feature set: [13, 18, 2]\n",
      "Testing feature set: [13, 18, 3]\n",
      "Testing feature set: [13, 18, 4]\n",
      "Testing feature set: [13, 18, 5]\n",
      "Testing feature set: [13, 18, 6]\n",
      "Testing feature set: [13, 18, 7]\n",
      "Testing feature set: [13, 18, 8]\n",
      "Testing feature set: [13, 18, 9]\n",
      "Testing feature set: [13, 18, 10]\n",
      "Testing feature set: [13, 18, 11]\n",
      "Testing feature set: [13, 18, 12]\n",
      "Testing feature set: [13, 18, 14]\n",
      "Testing feature set: [13, 18, 15]\n",
      "Testing feature set: [13, 18, 16]\n",
      "Testing feature set: [13, 18, 17]\n",
      "Testing feature set: [13, 18, 19]\n",
      "Testing feature set: [13, 18, 20]\n",
      "Testing feature set: [13, 18, 21]\n"
     ]
    }
   ],
   "source": [
    "X_new = selector.fit_transform(X, y_best_ansatz)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IA_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
