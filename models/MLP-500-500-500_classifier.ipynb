{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "970fa61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "\n",
    "from copy import deepcopy\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a2b867d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_500_500_500_classifier(nn.Module):\n",
    "    def __init__(self, input_len):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer1 = nn.Linear(input_len, 500)  \n",
    "        self.relu = nn.ReLU()                          \n",
    "        self.hidden_layer_relu = nn.Sequential(\n",
    "            nn.Linear(500,500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500,500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500,500),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer2 = nn.Linear(500, 30)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.hidden_layer_relu(x)\n",
    "        logits = self.layer2(x)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1068e216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# abrindo os dados de treinamento\n",
    "df = pd.read_csv(\"./../ansatz_result/data.csv\")\n",
    "X = df.drop(columns=[\"target\"]).to_numpy()\n",
    "y = pd.DataFrame(df['target'].apply(ast.literal_eval).tolist()).to_numpy()\n",
    "\n",
    "y_best_ansatz = np.argmax(y, axis=1)  \n",
    "\n",
    "# criando o kfold\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9b2b5f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cf1a08dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementação de um critério de parada para o modelo parar no ponto \"ótimo\"\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0.0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.early_stop = False\n",
    "        \n",
    "    def __call__(self, val_loss):\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8930e5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Função para treinar um fold\n",
    "def train_fold(model, train_loader, val_loader, criterion, optimizer, n_epochs=1000000, patience = 10, min_delta = 0.0):\n",
    "    model.to(device)\n",
    "    early_stopping = EarlyStopping(patience=patience, min_delta=min_delta)\n",
    "    best_model = None\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # Treinamento\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            y_batch = y_batch.squeeze()  # Converte (batch_size, 1) para (batch_size,)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validação\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for X_val, y_val in val_loader:\n",
    "                X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "                y_val = y_val.squeeze()  # Garante que y_val é 1D\n",
    "                \n",
    "                outputs = model(X_val)\n",
    "                val_loss += criterion(outputs, y_val).item()  \n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                correct += (predicted == y_val).sum().item()\n",
    "                total += y_val.size(0)\n",
    "        \n",
    "        \n",
    "        val_acc = correct / total\n",
    "        val_loss /= len(val_loader)\n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{n_epochs} - Train Loss: {train_loss:.4f} - Val Loss: {val_loss:.4f} - Val Acc: {val_acc:.4f}')\n",
    "        \n",
    "        # Early Stopping\n",
    "        early_stopping(val_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "            \n",
    "        # Salvar melhor modelo\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_model = deepcopy(model.state_dict())\n",
    "    \n",
    "    return best_model, best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cf393d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 1/3 ===\n",
      "Epoch 1/1000000 - Train Loss: 3.1483 - Val Loss: 3.2008 - Val Acc: 0.3000\n",
      "Epoch 2/1000000 - Train Loss: 2.7183 - Val Loss: 2.8224 - Val Acc: 0.3000\n",
      "Epoch 3/1000000 - Train Loss: 2.4985 - Val Loss: 2.7137 - Val Acc: 0.2583\n",
      "Epoch 4/1000000 - Train Loss: 2.3608 - Val Loss: 2.7547 - Val Acc: 0.2583\n",
      "Epoch 5/1000000 - Train Loss: 2.2703 - Val Loss: 2.7003 - Val Acc: 0.2583\n",
      "Epoch 6/1000000 - Train Loss: 2.2668 - Val Loss: 2.7410 - Val Acc: 0.2583\n",
      "Epoch 7/1000000 - Train Loss: 2.2705 - Val Loss: 2.6900 - Val Acc: 0.2583\n",
      "Epoch 8/1000000 - Train Loss: 2.2368 - Val Loss: 2.6717 - Val Acc: 0.2583\n",
      "Epoch 9/1000000 - Train Loss: 2.2528 - Val Loss: 2.6982 - Val Acc: 0.2667\n",
      "Epoch 10/1000000 - Train Loss: 2.2007 - Val Loss: 2.6918 - Val Acc: 0.2667\n",
      "Epoch 11/1000000 - Train Loss: 2.2030 - Val Loss: 2.6928 - Val Acc: 0.3000\n",
      "Epoch 12/1000000 - Train Loss: 2.1722 - Val Loss: 2.7194 - Val Acc: 0.2750\n",
      "Epoch 13/1000000 - Train Loss: 2.1966 - Val Loss: 2.7307 - Val Acc: 0.2833\n",
      "Epoch 14/1000000 - Train Loss: 2.1948 - Val Loss: 2.6905 - Val Acc: 0.3000\n",
      "Epoch 15/1000000 - Train Loss: 2.1780 - Val Loss: 2.7311 - Val Acc: 0.2667\n",
      "Epoch 16/1000000 - Train Loss: 2.1411 - Val Loss: 2.8282 - Val Acc: 0.2750\n",
      "Epoch 17/1000000 - Train Loss: 2.1960 - Val Loss: 2.6893 - Val Acc: 0.2583\n",
      "Epoch 18/1000000 - Train Loss: 2.1443 - Val Loss: 2.7679 - Val Acc: 0.2917\n",
      "Epoch 19/1000000 - Train Loss: 2.1521 - Val Loss: 2.7082 - Val Acc: 0.2500\n",
      "Epoch 20/1000000 - Train Loss: 2.1927 - Val Loss: 2.7639 - Val Acc: 0.2667\n",
      "Epoch 21/1000000 - Train Loss: 2.1194 - Val Loss: 2.7172 - Val Acc: 0.2583\n",
      "Epoch 22/1000000 - Train Loss: 2.1318 - Val Loss: 2.7666 - Val Acc: 0.2917\n",
      "Epoch 23/1000000 - Train Loss: 2.1290 - Val Loss: 2.7984 - Val Acc: 0.3083\n",
      "Early stopping at epoch 23\n",
      "\n",
      "=== Fold 2/3 ===\n",
      "Epoch 1/1000000 - Train Loss: 3.1688 - Val Loss: 2.9659 - Val Acc: 0.2917\n",
      "Epoch 2/1000000 - Train Loss: 2.7281 - Val Loss: 2.6888 - Val Acc: 0.2917\n",
      "Epoch 3/1000000 - Train Loss: 2.5364 - Val Loss: 2.5562 - Val Acc: 0.3250\n",
      "Epoch 4/1000000 - Train Loss: 2.4532 - Val Loss: 2.5078 - Val Acc: 0.3250\n",
      "Epoch 5/1000000 - Train Loss: 2.4226 - Val Loss: 2.4817 - Val Acc: 0.3250\n",
      "Epoch 6/1000000 - Train Loss: 2.3734 - Val Loss: 2.4628 - Val Acc: 0.3250\n",
      "Epoch 7/1000000 - Train Loss: 2.3314 - Val Loss: 2.4566 - Val Acc: 0.3250\n",
      "Epoch 8/1000000 - Train Loss: 2.3340 - Val Loss: 2.4594 - Val Acc: 0.3250\n",
      "Epoch 9/1000000 - Train Loss: 2.2972 - Val Loss: 2.4504 - Val Acc: 0.3250\n",
      "Epoch 10/1000000 - Train Loss: 2.2592 - Val Loss: 2.4415 - Val Acc: 0.3250\n",
      "Epoch 11/1000000 - Train Loss: 2.2613 - Val Loss: 2.4525 - Val Acc: 0.3500\n",
      "Epoch 12/1000000 - Train Loss: 2.2395 - Val Loss: 2.4361 - Val Acc: 0.3250\n",
      "Epoch 13/1000000 - Train Loss: 2.2512 - Val Loss: 2.4581 - Val Acc: 0.3250\n",
      "Epoch 14/1000000 - Train Loss: 2.2391 - Val Loss: 2.4414 - Val Acc: 0.3667\n",
      "Epoch 15/1000000 - Train Loss: 2.2368 - Val Loss: 2.4435 - Val Acc: 0.3667\n",
      "Epoch 16/1000000 - Train Loss: 2.2059 - Val Loss: 2.4535 - Val Acc: 0.3750\n",
      "Epoch 17/1000000 - Train Loss: 2.2475 - Val Loss: 2.4339 - Val Acc: 0.3500\n",
      "Epoch 18/1000000 - Train Loss: 2.2224 - Val Loss: 2.5062 - Val Acc: 0.3417\n",
      "Epoch 19/1000000 - Train Loss: 2.2499 - Val Loss: 2.4868 - Val Acc: 0.3250\n",
      "Epoch 20/1000000 - Train Loss: 2.1939 - Val Loss: 2.4877 - Val Acc: 0.3500\n",
      "Epoch 21/1000000 - Train Loss: 2.2041 - Val Loss: 2.4800 - Val Acc: 0.3750\n",
      "Epoch 22/1000000 - Train Loss: 2.2426 - Val Loss: 2.4838 - Val Acc: 0.3667\n",
      "Epoch 23/1000000 - Train Loss: 2.1620 - Val Loss: 2.4862 - Val Acc: 0.3583\n",
      "Epoch 24/1000000 - Train Loss: 2.2058 - Val Loss: 2.4614 - Val Acc: 0.3500\n",
      "Epoch 25/1000000 - Train Loss: 2.1144 - Val Loss: 2.5584 - Val Acc: 0.3833\n",
      "Epoch 26/1000000 - Train Loss: 2.1387 - Val Loss: 2.4958 - Val Acc: 0.3750\n",
      "Epoch 27/1000000 - Train Loss: 2.1466 - Val Loss: 2.5445 - Val Acc: 0.3500\n",
      "Epoch 28/1000000 - Train Loss: 2.1704 - Val Loss: 2.5004 - Val Acc: 0.3500\n",
      "Epoch 29/1000000 - Train Loss: 2.1370 - Val Loss: 2.4912 - Val Acc: 0.3417\n",
      "Epoch 30/1000000 - Train Loss: 2.1929 - Val Loss: 2.4836 - Val Acc: 0.3583\n",
      "Epoch 31/1000000 - Train Loss: 2.1942 - Val Loss: 2.4625 - Val Acc: 0.3583\n",
      "Epoch 32/1000000 - Train Loss: 2.1745 - Val Loss: 2.5113 - Val Acc: 0.3500\n",
      "Early stopping at epoch 32\n",
      "\n",
      "=== Fold 3/3 ===\n",
      "Epoch 1/1000000 - Train Loss: 3.1342 - Val Loss: 2.8935 - Val Acc: 0.2833\n",
      "Epoch 2/1000000 - Train Loss: 2.6549 - Val Loss: 2.5352 - Val Acc: 0.3167\n",
      "Epoch 3/1000000 - Train Loss: 2.4636 - Val Loss: 2.4669 - Val Acc: 0.3417\n",
      "Epoch 4/1000000 - Train Loss: 2.4046 - Val Loss: 2.4912 - Val Acc: 0.3500\n",
      "Epoch 5/1000000 - Train Loss: 2.4008 - Val Loss: 2.4801 - Val Acc: 0.3417\n",
      "Epoch 6/1000000 - Train Loss: 2.3328 - Val Loss: 2.5463 - Val Acc: 0.2833\n",
      "Epoch 7/1000000 - Train Loss: 2.3344 - Val Loss: 2.5734 - Val Acc: 0.3167\n",
      "Epoch 8/1000000 - Train Loss: 2.2862 - Val Loss: 2.6063 - Val Acc: 0.2750\n",
      "Epoch 9/1000000 - Train Loss: 2.3144 - Val Loss: 2.6312 - Val Acc: 0.3000\n",
      "Epoch 10/1000000 - Train Loss: 2.2727 - Val Loss: 2.6026 - Val Acc: 0.3333\n",
      "Epoch 11/1000000 - Train Loss: 2.2738 - Val Loss: 2.6199 - Val Acc: 0.3250\n",
      "Epoch 12/1000000 - Train Loss: 2.2167 - Val Loss: 2.6646 - Val Acc: 0.3167\n",
      "Epoch 13/1000000 - Train Loss: 2.2383 - Val Loss: 2.6460 - Val Acc: 0.3333\n",
      "Epoch 14/1000000 - Train Loss: 2.2430 - Val Loss: 2.7201 - Val Acc: 0.3417\n",
      "Epoch 15/1000000 - Train Loss: 2.2374 - Val Loss: 2.6628 - Val Acc: 0.2750\n",
      "Epoch 16/1000000 - Train Loss: 2.1880 - Val Loss: 2.6504 - Val Acc: 0.3333\n",
      "Epoch 17/1000000 - Train Loss: 2.2499 - Val Loss: 2.7055 - Val Acc: 0.2750\n",
      "Epoch 18/1000000 - Train Loss: 2.2105 - Val Loss: 2.6557 - Val Acc: 0.3333\n",
      "Early stopping at epoch 18\n",
      "\n",
      "=== Resultados ===\n",
      "Acurácia média: 0.3444 ± 0.0342\n",
      "Acurácias por fold: [0.3, 0.38333333333333336, 0.35]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "    print(f\"\\n=== Fold {fold + 1}/{3} ===\")\n",
    "    \n",
    "    # Dividir e preparar dados\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y_best_ansatz[train_idx], y_best_ansatz[val_idx]\n",
    "    \n",
    "    train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.LongTensor(y_train))\n",
    "    val_dataset = TensorDataset(torch.FloatTensor(X_val), torch.LongTensor(y_val))\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "    \n",
    "    # Inicializar modelo e otimizador\n",
    "    model = MLP_500_500_500_classifier(22)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)  # L2 regularization\n",
    "    \n",
    "    # Treinar fold\n",
    "    best_model, best_acc = train_fold(model, train_loader, val_loader, criterion, optimizer, patience=15)\n",
    "    results.append(best_acc)\n",
    "    \n",
    "    # Salvar modelo se necessário\n",
    "    torch.save(best_model, f'./models_salvos/best_model_MLP-500-500-500-classifier_fold{fold+1}.pt')\n",
    "\n",
    "# 7. Resultados finais\n",
    "print(\"\\n=== Resultados ===\")\n",
    "print(f\"Acurácia média: {np.mean(results):.4f} ± {np.std(results):.4f}\")\n",
    "print(f\"Acurácias por fold: {results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e1a86776",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP_500_500_500_classifier(\n",
       "  (layer1): Linear(in_features=22, out_features=500, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (hidden_layer_relu): Sequential(\n",
       "    (0): Linear(in_features=500, out_features=500, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=500, out_features=500, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=500, out_features=500, bias=True)\n",
       "    (5): ReLU()\n",
       "  )\n",
       "  (layer2): Linear(in_features=500, out_features=30, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLP_500_500_500_classifier(22).to(device)\n",
    "#carregando o melhor fold treinado\n",
    "model.load_state_dict(torch.load('.//models_salvos/best_model_MLP-500-500-500-classifier_fold2.pt'))  # Substitua pelo caminho correto\n",
    "model.eval()  # Modo de avaliação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8ebe1db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 3.0814, -0.5331,  1.8702, -2.4175, -7.4428, -7.7259,  2.3016,  2.8937,\n",
      "        -0.8710, -4.5846, -2.9946,  1.4828,  1.7127,  1.5595,  2.0949, -7.3915,\n",
      "        -0.1745, -2.4360, -5.3278, -5.5369, -4.2518,  1.5561, -3.6381, -4.4087,\n",
      "        -3.5264, -0.4362, -5.5552, -4.5998, -5.5612, -2.2605])\n",
      "torch.Size([30])\n"
     ]
    }
   ],
   "source": [
    "input = X[6]\n",
    "input_tensor = torch.FloatTensor(input).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(input_tensor)\n",
    "    print(logits)\n",
    "    print(logits.shape)\n",
    "    probabilities = torch.softmax(logits, dim=0)  # Shape (1, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9a740aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.5091e-01, 6.7570e-03, 7.4731e-02, 1.0264e-03, 6.7438e-06, 5.0808e-06,\n",
      "        1.1504e-01, 2.0796e-01, 4.8194e-03, 1.1754e-04, 5.7638e-04, 5.0725e-02,\n",
      "        6.3835e-02, 5.4770e-02, 9.3552e-02, 7.0986e-06, 9.6714e-03, 1.0077e-03,\n",
      "        5.5902e-05, 4.5355e-05, 1.6396e-04, 5.4584e-02, 3.0286e-04, 1.4015e-04,\n",
      "        3.3865e-04, 7.4446e-03, 4.4530e-05, 1.1577e-04, 4.4266e-05, 1.2010e-03])\n"
     ]
    }
   ],
   "source": [
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cae8d161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores ansatzes e suas probabilidades:\n",
      "1º: Ansatz 0 - Probabilidade: 0.2509\n",
      "2º: Ansatz 7 - Probabilidade: 0.2080\n",
      "3º: Ansatz 6 - Probabilidade: 0.1150\n"
     ]
    }
   ],
   "source": [
    "# Obter índices e valores das top-k probabilidades (ex.: top-5)\n",
    "k = 3\n",
    "top_k_probs, top_k_indices = torch.topk(probabilities, k=k, dim=0)\n",
    "\n",
    "# Converter para numpy e exibir\n",
    "top_k_probs = top_k_probs.cpu().numpy().flatten()\n",
    "top_k_indices = top_k_indices.cpu().numpy().flatten()\n",
    "\n",
    "print(\"Melhores ansatzes e suas probabilidades:\")\n",
    "for i, (idx, prob) in enumerate(zip(top_k_indices, top_k_probs)):\n",
    "    print(f\"{i+1}º: Ansatz {idx} - Probabilidade: {prob:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IA_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
